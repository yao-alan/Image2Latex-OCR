{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import preprocessing as pre\n",
    "import dataset as ds\n",
    "import utils\n",
    "from vocab import Vocab, label_to_index, indices_to_latex\n",
    "from model import CRNN, init_weights\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GET TRAIN/VAL/TEST SPLIT** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './data'\n",
    "data_folders = ['IM2LATEX-100K'] #,'IM2LATEX-100K-HANDWRITTEN']\n",
    "img_folders = ['formula_images'] #, 'images']\n",
    "\n",
    "# imgs stored in data_root/data_folders/img_folders\n",
    "assert len(data_folders) == len(img_folders)\n",
    "\n",
    "# get test/train/val indices\n",
    "# change ftype if different image type used\n",
    "df_list = []\n",
    "for i in range(len(data_folders)):\n",
    "    print(f'{data_root}/{data_folders[i]}')\n",
    "    df = pre.data_to_df(f'{data_root}/{data_folders[i]}', max_entries=None)\n",
    "    df['dataset'] = data_folders[i]\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **APPEND LABELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_folders = ['im2latex_formulas.lst'] #, 'formulas.lst']\n",
    "\n",
    "for i, df in enumerate(df_list):\n",
    "    labels = pre.extract_labels(f'{data_root}/{data_folders[i]}/{label_folders[i]}')\n",
    "    df['label'] = labels.loc[df['index']].values\n",
    "    df['label_token_indices'] = np.nan\n",
    "    # image shape after being processed\n",
    "    df['padded_height'] = np.nan\n",
    "    df['padded_width'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MERGE AND SHUFFLE DATAFRAMES** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_shuffle_df = pd.concat(df_list).sample(frac=1).reset_index(drop=True).astype('object')\n",
    "merge_shuffle_df.to_csv('../saved/merge_shuffle_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *If loading from saved df* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_shuffle_df = pd.read_csv('../saved/merge_shuffle_df.csv').astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TOKENIZE LATEX, CREATE FULL DATAFRAME** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = [\n",
    "    (40, 160), (40, 200), (40, 240), (40, 280), (40, 320), \\\n",
    "    (50, 120), (50, 200), (50, 240), (50, 280), (50, 320)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_images(img_path, aspect_ratios=None, image_sizes=None, reshape_strat='pad'):\n",
    "    \"\"\"Scale images to proper shape.\"\"\"\n",
    "    _, img = crop_equations(img_path)\n",
    "        \n",
    "    # figure out best padding\n",
    "    pad_height, pad_width = None, None\n",
    "    if image_sizes is not None:\n",
    "        # pad with 8 px to all sides\n",
    "        img = np.pad(img, pad_width=8, mode='constant', constant_values=255)\n",
    "\n",
    "        # scale to certain aspect ratio\n",
    "        if reshape_strat == 'scale':\n",
    "            # look for best aspect ratio\n",
    "            # second element of tuple is irrelevant\n",
    "            curr_ratio = img.shape[1] / img.shape[0]\n",
    "            idx = bisect.bisect_left(aspect_ratios, (curr_ratio, -1))\n",
    "            reshape_to = None\n",
    "            if idx == 0:\n",
    "                reshape_to = 0\n",
    "            elif idx == len(aspect_ratios):\n",
    "                reshape_to = len(aspect_ratios)-1\n",
    "            else:\n",
    "                if curr_ratio / aspect_ratios[idx-1] <= aspect_ratios[idx] / curr_ratio:\n",
    "                    reshape_to = idx-1\n",
    "                else:\n",
    "                    reshape_to = idx\n",
    "            img = cv2.resize(\n",
    "                img, \n",
    "                dsize=(aspect_ratios[reshape_to][1], aspect_ratios[reshape_to][0]),\n",
    "                interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "            pad_height = aspect_ratios[reshape_to][0]\n",
    "            pad_width = aspect_ratios[reshape_to][1]\n",
    "        # pad to fit image size\n",
    "        elif reshape_strat == 'pad':\n",
    "            img = cv2.resize(\n",
    "                img, \n",
    "                dsize=(\n",
    "                    min(int(img.shape[1]/2), 320),\n",
    "                    min(int(img.shape[0]/2), 50)\n",
    "                ),\n",
    "                interpolation=cv2.INTER_AREA\n",
    "            )\n",
    "            # plt.imshow(img)\n",
    "            # plt.show()\n",
    "            # figure out which shape to pad to\n",
    "            pad_dh, pad_dw = np.inf, np.inf\n",
    "            for _, (height, width) in enumerate(image_sizes):\n",
    "                dh = height-img.shape[0]\n",
    "                dw = width-img.shape[1]\n",
    "                if 0 <= dh and dh <= pad_dh and 0 <= dw and dw <= pad_dw:\n",
    "                    pad_dh = height-img.shape[0]\n",
    "                    pad_dw = width-img.shape[1]\n",
    "                    pad_height = height\n",
    "                    pad_width = width\n",
    "            # if odd padding, top padded less than bottom and left less than right\n",
    "            top = int(pad_dh/2)\n",
    "            left = int(pad_dw/2)\n",
    "            img = np.pad(\n",
    "                img,\n",
    "                pad_width=((top, pad_dh-top), (left, pad_dw-left)),\n",
    "                mode='constant',\n",
    "                constant_values=255\n",
    "            )\n",
    "\n",
    "    img = np.abs(255-img)\n",
    "    img = np.expand_dims(img, axis=(0))\n",
    "\n",
    "    return img, pad_height, pad_width\n",
    "\n",
    "\n",
    "# image_sizes is a list of tuples(height, width) to try to fit to (if batching)\n",
    "# reshape_strat = 'scale', 'pad', 'None'\n",
    "def scale_images(merge_shuffle_df, maxlen=None, image_sizes=None, \n",
    "                 reshape_strat='pad', model=None):\n",
    "    \"\"\"Scale images to proper shape and create proper labels.\"\"\"\n",
    "    v = Vocab()\n",
    "    images = {}\n",
    "    # Harvard's image sizes:\n",
    "    #   (40, 160), (40, 200), (40, 240), (40, 280), (40, 320)\n",
    "    #   (50, 120), (50, 200), (50, 240), (50, 280)\n",
    "    aspect_ratios = []\n",
    "    seq_len_dict = {}\n",
    "    if image_sizes is not None:\n",
    "        for i, (height, width) in enumerate(image_sizes):\n",
    "            aspect_ratios.append((width / height, i))\n",
    "            if maxlen is None:\n",
    "                assert model is not None\n",
    "                seq_len_dict[(height, width)] = model.get_seq_len(height, width)\n",
    "        aspect_ratios.sort()\n",
    "\n",
    "    for i, row in merge_shuffle_df.iterrows():\n",
    "        # remove \\label{...} and tokenize\n",
    "        #label = tokenize(re.sub(r'\\\\label\\{.*\\}', '', row['label']))\n",
    "        label = pre.tokenize(row['label'])\n",
    "        # print(label)\n",
    "        v.update(label)\n",
    "\n",
    "        img, pad_height, pad_width = reshape_images(\n",
    "            row['image_path'], aspect_ratios, image_sizes, reshape_strat\n",
    "        )\n",
    "\n",
    "        merge_shuffle_df.at[i, 'label'] = label\n",
    "        merge_shuffle_df.at[i, 'padded_height'] = pad_height\n",
    "        merge_shuffle_df.at[i, 'padded_width'] = pad_width\n",
    "        images[(row['index'], row['dataset'])] = img\n",
    "        \n",
    "        print(f'{i+1}/{len(merge_shuffle_df)}')\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    for i, row in merge_shuffle_df.iterrows():\n",
    "        if maxlen is None:\n",
    "            maxlen = None if image_sizes is None \\\n",
    "                else seq_len_dict[(row['padded_height'], row['padded_width'])]\n",
    "        merge_shuffle_df.at[i, 'label_token_indices'] = np.array(\n",
    "            label_to_index(row['label'], v, maxlen=maxlen)\n",
    "        )\n",
    "        \n",
    "        print(f'{i+1}/{len(merge_shuffle_df)}')\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    return merge_shuffle_df, images, v\n",
    "\n",
    "\n",
    "def crop_equations(path, show_image=False):\n",
    "    \"\"\"Simplistic method to isolate the singular equation in the image.\"\"\"\n",
    "    img = cv2.imread(path, 0)\n",
    "\n",
    "    LR_mask = cv2.reduce(img, 0, cv2.REDUCE_MIN) != 255\n",
    "    UD_mask = cv2.reduce(img, 1, cv2.REDUCE_MIN) != 255\n",
    "    c1, c2 = LR_mask.argmax(), LR_mask.shape[1] - np.flip(LR_mask).argmax()\n",
    "    r1, r2 = UD_mask.argmax(), UD_mask.shape[0] - np.flip(UD_mask).argmax()\n",
    "\n",
    "    if show_image:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    return path, img[r1:r2, c1:c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_shuffle_df, images, vocab = scale_images(\n",
    "    merge_shuffle_df,\n",
    "    maxlen=200,\n",
    "    image_sizes=image_sizes,\n",
    "    reshape_strat='pad'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_shuffle_df.to_csv('../saved/merge_shuffle_df_processed.csv')\n",
    "image_handler = open('../saved/images', 'wb')\n",
    "pickle.dump(images, image_handler)\n",
    "vocab_handler = open('../saved/vocab', 'wb')\n",
    "pickle.dump(vocab, vocab_handler)\n",
    "image_handler.close()\n",
    "vocab_handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Load items in if everything is saved* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_shuffle_df = pd.read_csv('../saved/merge_shuffle_df_processed.csv')\n",
    "images = pickle.load(open('../saved/images', 'rb'))\n",
    "vocab = Vocab()\n",
    "vocab = pickle.load(open('../saved/vocab', 'rb'))\n",
    "\n",
    "merge_shuffle_df = merge_shuffle_df.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in merge_shuffle_df.index:\n",
    "    clear_output(wait=True)\n",
    "    print(idx)\n",
    "    merge_shuffle_df.at[idx, 'label'] \\\n",
    "        = [x[1:-1] for x in np.asarray(merge_shuffle_df['label'].iloc[idx][1:-1].split(', '))]\n",
    "    merge_shuffle_df.at[idx, 'label_token_indices'] \\\n",
    "        = np.array(merge_shuffle_df['label_token_indices'].iloc[idx][1:-1].split(), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CREATE DATALOADERS** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders = [\n",
    "    ds.gen_dataloader(\n",
    "        merge_shuffle_df=merge_shuffle_df,\n",
    "        images=images,\n",
    "        split='train',\n",
    "        extra_cond=((merge_shuffle_df['padded_height'] == h) & (merge_shuffle_df['padded_width'] == w)),\n",
    "        batch_size=20, \n",
    "        shuffle=True,\n",
    "        merge_train_validate=True\n",
    "    ) for (h, w) in image_sizes\n",
    "]\n",
    "\n",
    "test_dataloaders = [\n",
    "    ds.gen_dataloader(\n",
    "        merge_shuffle_df=merge_shuffle_df,\n",
    "        images=images,\n",
    "        split='test',\n",
    "        extra_cond=((merge_shuffle_df['padded_height'] == h) & (merge_shuffle_df['padded_width'] == w)),\n",
    "        batch_size=20, \n",
    "        shuffle=True,\n",
    "        merge_train_validate=True\n",
    "    ) for (h, w) in image_sizes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CONSTRUCT MODEL** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(vocab, embed_size=32)\n",
    "model.apply(init_weights)\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *If there are trained parameters to load in* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../saved/params/CRNN_params_epoch_5.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders is a list\n",
    "def train_loop(model, train_dataloaders, vocab, num_epochs, lr=0.1, test_dataloaders=None, main_device='cpu'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-6)\n",
    "    ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    ctc_loss = nn.CTCLoss(blank=vocab.get_index('<nul>'), reduction='none', zero_infinity=True)\n",
    "    animator = utils.Animator(\n",
    "        measurement_names=['ctc_loss'],\n",
    "        refresh=1\n",
    "    )\n",
    "    metrics = utils.MetricBuffer(['sum_ctc_loss', 'n_tokens'])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        n_imgs = 0\n",
    "        for dataloader in train_dataloaders:\n",
    "            for i, (imgs, labels) in enumerate(dataloader):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                imgs = torch.tensor(imgs).to(main_device).float()\n",
    "                labels = labels.to(main_device).long()\n",
    "                output = model(imgs, teacher_forcing=False)\n",
    "                # output.shape = (seq_len, batch_size, vocab_size)\n",
    "\n",
    "                # for CTCLoss\n",
    "                log_softmax_probs = F.log_softmax(output, dim=2)\n",
    "                labels = labels[:, 1:log_softmax_probs.shape[0]+1]\n",
    "                # requires shape (seq_len, batch_size, vocab_size)\n",
    "                print(log_softmax_probs.shape, labels.shape, (torch.ones((labels.shape[0])) * labels.shape[1]).shape)\n",
    "                main_loss = ctc_loss(\n",
    "                    log_softmax_probs,\n",
    "                    labels, \n",
    "                    (torch.ones((labels.shape[0])) * labels.shape[1]).int().to(output.device),\n",
    "                    (labels != vocab.get_index('<pad>')).sum(dim=1).int().to(output.device)\n",
    "                )\n",
    "\n",
    "                # for CrossEntropyLoss\n",
    "                second_loss = ce_loss(\n",
    "                    output.permute(1, 2, 0),\n",
    "                    labels\n",
    "                )\n",
    "\n",
    "                main_loss.sum().backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                optimizer.step()\n",
    "\n",
    "                metrics.update([('sum_ctc_loss', main_loss.sum().cpu().detach().numpy())])\n",
    "                metrics.update([('n_tokens', (labels != vocab.get_index('<pad>')).sum(dim=[0, 1]).cpu().detach().numpy())])\n",
    "\n",
    "                n_imgs += imgs.shape[0]\n",
    "                # print(n_imgs)\n",
    "                \n",
    "                print(i)\n",
    "                if i % 10 == 0:\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"n_imgs #{n_imgs}\")\n",
    "                    print(f\"Current loss: {metrics['sum_ctc_loss'] / metrics['n_tokens']}\")\n",
    "                    # try printing from test set\n",
    "                    #for test_imgs, test_labels in gen_dataloader('train', batch_size=2, shuffle=True):\n",
    "                    if test_dataloaders is not None:\n",
    "                        test_loader = test_dataloaders[np.random.randint(0, len(test_dataloaders))]\n",
    "                        for test_imgs, test_labels in test_loader:\n",
    "                            model.eval()\n",
    "                            with torch.no_grad():\n",
    "                                output = model(test_imgs.to(main_device).float())\n",
    "                                print(output)\n",
    "                                seq_list = indices_to_latex(output, vocab)\n",
    "                                label_list = indices_to_latex(test_labels.int(), vocab)\n",
    "                                print(len(seq_list))\n",
    "                                for i, seq in enumerate(seq_list):\n",
    "                                    plt.imshow(test_imgs[i, 0])\n",
    "                                    plt.show()\n",
    "                                    print(\"Prediction:\")\n",
    "                                    print(seq)\n",
    "                                    print(\"Real:\")\n",
    "                                    print(label_list[i])\n",
    "                                    print()\n",
    "                                    print()\n",
    "                                break\n",
    "\n",
    "            \n",
    "        animator.append([\n",
    "            ('ctc_loss', metrics['sum_ctc_loss'] / metrics['n_tokens']),\n",
    "        ])\n",
    "\n",
    "        metrics.clear()\n",
    "        torch.save(model.state_dict(), f'../saved/params/CRNN_params_epoch_{epoch+1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loop(model, train_dataloaders, vocab, num_epochs=5, lr=3e-4, test_dataloaders=test_dataloaders, main_device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUATE MODEL** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_bleu = 0\n",
    "sum_edit = 0\n",
    "n_examples = 0\n",
    "for test_loader in test_dataloaders:\n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(test_imgs.to('cuda:0').float())\n",
    "            seq_list = indices_to_latex(output, vocab)\n",
    "            label_list = indices_to_latex(test_labels.int(), vocab, using_ctc_loss=False)\n",
    "            for i, seq in enumerate(seq_list):\n",
    "                plt.imshow(test_imgs[i, 0])\n",
    "                plt.show()\n",
    "                bleu_score = nltk.translate.bleu_score.sentence_bleu([label_list[i]], seq)\n",
    "                edit_dist = nltk.edit_distance(label_list[i], seq)\n",
    "                sum_bleu += bleu_score\n",
    "                sum_edit += edit_dist\n",
    "                n_examples += 1\n",
    "                print(\"Prediction:\")\n",
    "                print(seq)\n",
    "                print(\"Real:\")\n",
    "                print(label_list[i])\n",
    "                print(\"BLEU:\")\n",
    "                print(bleu_score)\n",
    "                print(\"Edit:\")\n",
    "                print(edit_dist)\n",
    "                print()\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f\"Avg. BLEU-4: {sum_bleu / n_examples}\")\n",
    "print(f\"Avg. edit distance: {sum_edit / n_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "be8b961c1e7d1263c9983ef5ae9a574c2381475363281d9ee8df40a058335103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
